use chrono::{DateTime, Utc};
use anyhow::Result;
use tokio_postgres::Transaction;
use crate::connect::Target;

static DDL: &str = r#"
create schema __backfill;

/*
create table __backfill.setting
( var text not null
, val text
, unique var include (val)
);
*/

create table __backfill.task
( id bigint not null generated by default as identity primary key
, chunk_id int not null unique
, worked tstzrange
);
create index on __backfill.copy_chunk_task (priority) include (chunk_id) where (worked is null);
"#;

pub async fn init(target: &mut Target) -> Result<()> {
    let tx = target.client.transaction().await?;
    let row = tx.query_one("select count(*) > 0 from pg_namespace where nspname = '__backfill'", &[]).await?;
    let schema_exists: bool = row.get(0);
    if !schema_exists {
        tx.execute(DDL, &[until]).await?;
        tx.commit().await?;
    } else {
        tx.rollback().await?;
    }
    Ok(())
}

/*
static UPSERT_SETTING: &str = r#"
insert into __backfill.setting set val = $2
where var = $1
on conflict (var) do update set val = EXCLUDED.val
"#;

pub async fn set_consistency_time(
    target: &mut Target,
    until: &DateTime<Utc>
) -> Result<()> {
    target.client.execute(UPSERT_SETTING, &["consistency_time", until]).await?;
    Ok(())
}
*/

static LOAD_QUEUE_QUERY: &str = r#"
insert into __backfill.task (chunk_id)
select c.chunk_id
from _timescaledb_catalog.chunk c
where c.dropped is false
and not exists
(
    select 1
    from _timescaledb_catalog.chunk c2
    where c2.compressed_chunk_id = c.id
)
on conflict (chunk_id) do nothing
"#;

pub async fn load_queue(target: &mut Target) -> Result<()> {
    target.client.execute(LOAD_QUEUE_QUERY, &[]).await?;
    Ok(())
}

#[derive(Debug)]
pub struct Task {
    pub id: i64,
    pub chunk_id: i64,
    pub chunk_schema: String,
    pub chunk_name: String,
    pub hypertable_id: i64,
    pub hypertable_schema: String,
    pub hypertable_name: String,
    pub compressed_chunk_id: Option<i64>,
    pub compressed_chunk_schema: Option<String>,
    pub compressed_chunk_name: Option<String>,
    pub filter: Option<String>,
}

static CLAIM_TASK: &str = r#"
select id, chunk_id
from __backfill.task
where worked is null
order by id
for update skip locked
limit 1
"#;

static TASK_DETAILS: &str = r#"
select
  $1::bigint as id
, c.id as chunk_id
, c.schema_name as chunk_schema
, c.table_name as chunk_name
, h.id as hypertable_id
, h.schema_name as hypertable_schema
, h.table_name as hypertable_name
--, d.column_name
--, d.column_type
--, ds.range_start
--, ds.range_end
, c2.id as compressed_chunk_id
, c2.schema_name as compressed_chunk_schema
, c2.table_name as compressed_chunk_name
, case d.column_type
    when 'timestamptz'::regtype then
        case
            when _timescaledb_internal.to_timestamp(ds.range_start) < $3
            and $3 < _timescaledb_internal.to_timestamp(ds.range_end)
            then
            format($sql$where %I < %L$sql$, d.column_name, $3)
        end
    when 'timestamp'::regtype then
        case
            when _timescaledb_internal.to_timestamp_without_timezone(ds.range_start) < $3::timestamp
            and $3::timestamp < _timescaledb_internal.to_timestamp_without_timezone(ds.range_end)
            then
            format($sql$where %I < %L$sql$, d.column_name, $3::timestamp)
        end
    when 'date'::regtype then
        case
            when _timescaledb_internal.to_date(ds.range_start) < $3::date
            and $3::date < _timescaledb_internal.to_date(ds.range_end)
            then
            format($sql$where %I < %L$sql$, d.column_name, $3::date)
        end
    when 'bigint'::regtype then
        case
            when ds.range_start < _timescaledb_internal.to_unix_microseconds($3)
            and _timescaledb_internal.to_unix_microseconds($3) < ds.range_end
            then
            format($sql$where %I < %L$sql$, d.column_name, _timescaledb_internal.to_unix_microseconds($3::date))
        end
  end as filter
from _timescaledb_catalog.chunk c
inner join _timescaledb_catalog.hypertable h on (c.hypertable_id = h.id)
inner join lateral
(
    select d.id, d.column_name, d.column_type
    from _timescaledb_catalog.dimension d
    where h.id = d.hypertable_id
    order by d.id
    limit 1
) d on (true)
inner join _timescaledb_catalog.chunk_constraint cc on (c.id = cc.chunk_id)
inner join _timescaledb_catalog.dimension_slice ds on (cc.dimension_slice_id = ds.id and ds.dimension_id = d.id)
left outer join _timescaledb_catalog.chunk c2 on (c.compressed_chunk_id = c2.id)
where c.id = $2
and case d.column_type
        when 'timestamptz'::regtype then _timescaledb_internal.to_timestamp(ds.range_start) < $3
        when 'timestamp'::regtype then _timescaledb_internal.to_timestamp_without_timezone(ds.range_start) < $3::timestamp
        when 'date'::regtype then _timescaledb_internal.to_date(ds.range_start) < $3::date
        when 'bigint'::regtype then ds.range_start < _timescaledb_internal.to_unix_microseconds($3)
        else false
    end
;
"#;

pub async fn claim_task(target_tx: &Transaction<'_>, until: &DateTime<Utc>) -> Option<Task> {
    match target_tx.query_opt(POP_TASK, &[]).await? {
        Some(row) => {
            let task_id: i64 = row.get(0);
            let chunk_id: i64 = row.get(1);
            match target_tx.query_opt(TASK_DETAILS, &[task_id, chunk_id, until]).await? {
                Some(row) => {
                    Option(Task{
                        id: row.get("id"),
                        chunk_id: row.get("chunk_id"),
                        chunk_schema: row.get("chunk_schema"),
                        chunk_name: row.get("chunk_name"),
                        hypertable_id: row.get("hypertable_id"),
                        hypertable_schema: row.get("hypertable_schema"),
                        hypertable_name: row.get("hypertable_name"),
                        compressed_chunk_id: row.get("compressed_chunk_id"),
                        compressed_chunk_schema: row.get("compressed_chunk_schema"),
                        compressed_chunk_name: row.get("compressed_chunk_name"),
                        filter: row.get("filter"),
                    })
                },
                None => None
            }
        },
        None => None
    }
}

static COMPLETE_TASK: &str = r#"
update __backfill.task set worked = tstzrange(now(), clock_timestamp())
where id = $1
"#;

pub async fn complete_task(target_tx: &Transaction<'_>, id: i64) -> Result<()> {
    target_tx.execute(COMPLETE_TASK, &[id]).await?;
    Ok(())
}